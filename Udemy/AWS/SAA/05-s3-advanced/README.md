# S3 avanzado

---

## üßä Movimiento entre clases de almacenamiento en S3

Amazon S3 ofrece **varias clases de almacenamiento**, dise√±adas para diferentes patrones de acceso y requisitos de costo. Puedes mover objetos entre ellas **autom√°ticamente** mediante reglas de **ciclo de vida (Lifecycle rules)**.

---

### üì¶ Principales clases de almacenamiento

| Clase                       | Descripci√≥n                                                     | Uso ideal                         |
| --------------------------- | --------------------------------------------------------------- | --------------------------------- |
| **S3 Standard**             | Alta disponibilidad, baja latencia                              | Archivos accedidos frecuentemente |
| **S3 Standard-IA**          | Infrequent Access: bajo costo, pero con tarifa por recuperaci√≥n | Archivos accedidos ocasionalmente |
| **S3 One Zone-IA**          | Similar a Standard-IA, pero solo en 1 AZ                        | Backups no cr√≠ticos               |
| **S3 Glacier**              | Archivo con acceso en minutos u horas                           | Archivado de datos a largo plazo  |
| **S3 Glacier Deep Archive** | Acceso en 12-48h, m√°s econ√≥mico                                 | Archivado casi hist√≥rico          |
| **S3 Intelligent-Tiering**  | Mueve objetos autom√°ticamente seg√∫n patrones de acceso          | Datos con accesos impredecibles   |

---

## üîÅ Transiciones de clase (Lifecycle Transitions)

Puedes configurar reglas que **transicionen objetos de una clase a otra** despu√©s de un n√∫mero de d√≠as.

### üß† Ejemplo t√≠pico

1. **D√≠a 0:** Objeto guardado en **S3 Standard**
2. **D√≠a 60:** Mover a **S3 Standard-IA**
3. **D√≠a 180:** Mover a **S3 Glacier**
4. **D√≠a 365:** Eliminar el objeto (opcional)

---

## üîÑ Reglas del ciclo de vida (Lifecycle Rules)

### üìå ¬øQu√© pueden hacer?

- **Transicionar objetos entre clases** seg√∫n edad.
- **Eliminar objetos autom√°ticamente** (expiraci√≥n).
- **Aplicarse a todo el bucket o a subconjuntos** por:

  - **Prefijo** (por ejemplo: `logs/`)
  - **Etiquetas (tags)** (por ejemplo: `{"archivado": "s√≠"}`)

### üìò Sintaxis com√∫n en consola

```plaintext
Transici√≥n:
- Si objeto tiene m√°s de 60 d√≠as ‚Üí IA
- Si objeto tiene m√°s de 180 d√≠as ‚Üí Glacier

Expiraci√≥n:
- Si objeto tiene m√°s de 365 d√≠as ‚Üí eliminar
```

---

## üß™ Casos de uso comunes

| Caso                           | Clase de destino     |
| ------------------------------ | -------------------- |
| Logs que se consultan poco     | IA o Glacier         |
| Archivos de cumplimiento legal | Glacier Deep Archive |
| Cach√©s est√°ticas               | S3 Standard          |
| Backups diarios                | IA o One Zone-IA     |

---

## üí° Recomendaciones

- Usa **tags** para tener reglas espec√≠ficas sin afectar todo el bucket.
- Glacier y Deep Archive no son instant√°neos: ten presente los **tiempos de recuperaci√≥n**.
- S3 Intelligent-Tiering puede ayudarte si **no sabes con certeza** c√≥mo se acceder√°n los datos.

---

## üìã Resumen

| Elemento                 | Detalle                                                           |
| ------------------------ | ----------------------------------------------------------------- |
| Clases de almacenamiento | 6 principales, cada una con casos de uso                          |
| Movimiento entre clases  | Se hace con **acciones de transici√≥n** en reglas de ciclo de vida |
| Eliminaci√≥n autom√°tica   | Se hace con **acciones de expiraci√≥n**                            |
| Aplicaci√≥n de reglas     | Por **prefijo**, **tag** o global                                 |
| Costo eficiente          | Reduce costos al mover archivos inactivos a clases m√°s baratas    |
| Automatizable            | ‚úÖ 100% con lifecycle rules y sin necesidad de scripts externos   |

## üìä Amazon S3 Storage Class Analysis (Analytics)

### üéØ ¬øPara qu√© sirve?

**S3 Analytics** te ayuda a analizar los patrones de acceso a los objetos almacenados en un bucket para decidir **cu√°ndo moverlos autom√°ticamente a una clase de almacenamiento m√°s econ√≥mica**, como **S3 Standard-IA**.

> Es √∫til especialmente cuando **no est√°s seguro de la frecuencia de acceso a tus datos**.

---

## ‚úÖ Casos de uso ideal

- Buckets con objetos en **S3 Standard** que podr√≠an beneficiarse del cambio a **S3 Standard-IA**.
- An√°lisis previo antes de configurar reglas de ciclo de vida.
- Proyectos con gran volumen de datos y comportamiento variable.

---

## üß† C√≥mo funciona

1. **Activas Analytics** en un bucket o prefijo.
2. AWS comienza a recopilar datos de acceso.
3. Despu√©s de **24 a 48 horas**, empiezas a recibir reportes.
4. Se actualiza **diariamente** y muestra:

   - Bytes almacenados
   - N√∫mero de objetos
   - Frecuencia de acceso
   - Porcentaje de acceso reciente vs hist√≥rico

5. T√∫ decides si mover esos objetos a IA o no.

---

## üìÅ Aplicabilidad

- Puedes aplicar el an√°lisis a **todo el bucket**, a **un prefijo espec√≠fico** o a **objetos con ciertas etiquetas (tags)**.
- Esto te da granularidad: puedes analizar solo los objetos que te interesan.

---

## üìâ Ejemplo de an√°lisis

Supongamos que tienes un bucket con 100 GB de datos. S3 Analytics puede mostrarte:

| Prefijo      | Tama√±o | % Accedido √∫ltimos 30 d√≠as | ¬øConviene IA? |
| ------------ | ------ | -------------------------- | ------------- |
| `/logs/`     | 30 GB  | 2%                         | ‚úÖ            |
| `/images/`   | 50 GB  | 85%                        | ‚ùå            |
| `/archivos/` | 20 GB  | 5%                         | ‚úÖ            |

---

## üß© Integraci√≥n con reglas de ciclo de vida

Una vez identificados los datos que no se acceden frecuentemente, puedes:

- Crear reglas de **transici√≥n** a **IA o Glacier**
- Crear reglas de **expiraci√≥n**
- Aplicar reglas a los prefijos analizados

> üìå **S3 Analytics no realiza la transici√≥n autom√°ticamente**, pero **es un primer paso crucial**.

---

## üõë Limitaciones

| Limitaci√≥n        | Detalle                                                                                        |
| ----------------- | ---------------------------------------------------------------------------------------------- |
| Tiempo inicial    | Tarda **24-48h** en generar los primeros informes                                              |
| Clases soportadas | Solo compara entre **S3 Standard** y **S3 Standard-IA**                                        |
| Costo             | **Gratuito** dentro del servicio de S3, pero hay cargos normales por almacenamiento y requests |
| No analiza        | Glacier, Deep Archive, Intelligent-Tiering, etc.                                               |

---

## üìã Resumen de s3 analytics

| Tema                   | Detalle                                          |
| ---------------------- | ------------------------------------------------ |
| Qu√© es                 | Herramienta para analizar el acceso a objetos    |
| Objetivo               | Ayudar a decidir si conviene mover a Standard-IA |
| Frecuencia de an√°lisis | Diario                                           |
| Tiempo inicial         | Primer informe: 24-48 horas                      |
| Aplicable por          | Bucket, prefijo, o etiqueta                      |
| Ideal para             | Crear reglas de ciclo de vida informadas         |
| Automatizaci√≥n         | No realiza cambios, solo analiza                 |

## üí∞ S3 ‚Äú**Requester Pays**‚Äù buckets

| Concepto                     | Detalle                                                                                                                                                                                                                         |
| ---------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **¬øQu√© resuelve?**           | El **propietario** de un bucket paga siempre el **almacenamiento**. Con _Requester Pays_ el **coste de las peticiones y de la transferencia de datos** (GET, LIST, PUT, egress) lo asume **quien descarga o sube los objetos**. |
| **Cu√°ndo usarlo**            | - Compartir grandes datasets (logs, im√°genes satelitales, genomics‚Ä¶) sin cargar al due√±o.- Repositorios p√∫blicos de investigaci√≥n donde cada equipo cubre su propio ancho de banda.                                             |
| **Requisitos**               | üîê El solicitante **debe autenticarse** con una cuenta AWS (no funciona para acceso an√≥nimo).üîë La **pol√≠tica**/ACL debe autorizar la acci√≥n solicitada.üåê Aplica tanto a Internet como a VPC Endpoints.                        |
| **Qui√©n paga qu√©**           | Propietario ‚áí _GB-mes de almacenamiento_.Solicitante ‚áí _Requests_, **Data Transfer OUT**, **Data Transfer IN** (si aplica).                                                                                                     |
| **Clases de almacenamiento** | Funciona con todas; los recargos (ej. Glacier retrieval) tambi√©n los paga el solicitante.                                                                                                                                       |

---

### üîß C√≥mo activarlo

```bash
aws s3api put-bucket-request-payment \
  --bucket my-dataset-bucket \
  --request-payment-configuration Payer=Requester
```

_Para comprobar:_

```bash
aws s3api get-bucket-request-payment --bucket my-dataset-bucket
```

---

### üõÇ Pol√≠tica m√≠nima de acceso (ejemplo)

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "ReadOnlyRequesterPays",
      "Effect": "Allow",
      "Principal": "*",
      "Action": ["s3:GetObject"],
      "Resource": "arn:aws:s3:::my-dataset-bucket/*"
    }
  ]
}
```

> Aunque la pol√≠tica permita `*`, cada requester **debe** usar `--request-payer requester` (CLI/SDK) para que la operaci√≥n prospere y se le facture.

---

### üß™ Uso desde CLI por parte del solicitante

```bash
aws s3 cp s3://my-dataset-bucket/bigfile.csv ./ \
  --request-payer requester
```

Sin el flag, la llamada devuelve **403 Access Denied**.

---

### ‚ö†Ô∏è Consideraciones

- **Costes inesperados**: el propietario sigue pagando el almacenamiento diario.
- **Logging**: habilita Server Access Logs o CloudTrail para auditor√≠a.
- **Cross-Account**: combina _Requester Pays_ con **Bucket Policy** o **Access Points** para precisar permisos.
- **No caching de CloudFront gratuito**: si pones el bucket detr√°s de CloudFront, el due√±o del CloudFront paga el egress desde la edge, no el requester.

---

### üìù Resumen r√°pido

1. _Requester Pays_ = el **requester** paga **requests + transferencia**.
2. Propietario **siempre** paga **almacenamiento**.
3. Requiere **autenticaci√≥n AWS** y `--request-payer requester`.
4. Perfecto para compartir datasets voluminosos sin facturas sorpresa.

## üì£ Notificaciones de eventos de Amazon S3

### üéØ ¬øQu√© son?

Permiten a S3 **notificar autom√°ticamente** cuando ocurren ciertos eventos sobre los objetos en un bucket, como:

- Subida (`s3:ObjectCreated:*`)
- Eliminaci√≥n (`s3:ObjectRemoved:*`)
- Restauraci√≥n desde Glacier
- Fallos de replicaci√≥n
- Cambios en el ciclo de vida

---

## üìç Destinos compatibles

| Destino         | ¬øPara qu√© sirve?                                                       | Caracter√≠sticas                                                   |
| --------------- | ---------------------------------------------------------------------- | ----------------------------------------------------------------- |
| **SNS**         | Publicar notificaciones a m√∫ltiples suscriptores (email, SMS, HTTP...) | Env√≠o masivo y fan-out                                            |
| **SQS**         | Encolar eventos para procesamiento posterior por consumidores          | Asegura el orden, desacopla procesamiento                         |
| **Lambda**      | Ejecutar l√≥gica directamente sobre un evento                           | Serverless, reactivo, c√≥digo en tiempo real                       |
| **EventBridge** | Motor de eventos con reglas avanzadas, m√∫ltiples destinos y fiabilidad | Filtros JSON, redirecci√≥n de eventos, almacenamiento y repetici√≥n |

---

## üîÅ Flujo t√≠pico

1. Se sube un archivo al bucket (ej. `PUT`).
2. S3 detecta el evento `ObjectCreated:Put`.
3. Env√≠a notificaci√≥n al destino configurado (ej. SQS, Lambda, etc).
4. El destino reacciona (procesa, transforma, almacena‚Ä¶).

---

## üîß Configuraci√≥n b√°sica (SQS, SNS, Lambda)

Se puede hacer por:

- **Consola**
- **AWS CLI**
- **S3 API** (v√≠a `NotificationConfiguration`)

```json
{
  "LambdaFunctionConfigurations": [
    {
      "LambdaFunctionArn": "arn:aws:lambda:region:account:function:processImage",
      "Events": ["s3:ObjectCreated:*"],
      "Filter": {
        "Key": {
          "FilterRules": [
            {
              "Name": "suffix",
              "Value": ".jpg"
            }
          ]
        }
      }
    }
  ]
}
```

> Tambi√©n puedes usar prefijos (`prefix`) para limitar eventos a carpetas espec√≠ficas.

---

## üî• Ventajas de usar EventBridge con S3

| Ventaja                           | Detalle                                                                                                |
| --------------------------------- | ------------------------------------------------------------------------------------------------------ |
| üéØ **Filtros potentes en JSON**   | Puedes hacer filtros por prefijo, sufijo, bucket name, regi√≥n, o campos personalizados                 |
| üö¶ **Ruteo avanzado**             | Puedes dirigir eventos a m√∫ltiples destinos: Lambda, Step Functions, Kinesis, SNS, SQS, otros EventBus |
| üïí **Reintento y entrega fiable** | Reintentos autom√°ticos y Dead Letter Queues                                                            |
| üîÑ **Repetici√≥n de eventos**      | Puedes volver a emitir eventos pasados almacenados en EventBridge                                      |
| üß© **Integraci√≥n con SaaS**       | Puedes capturar eventos de servicios SaaS como Datadog, Auth0, MongoDB Atlas                           |
| üóÇÔ∏è **Archivado de eventos**       | Puedes conservar eventos hist√≥ricos por tiempo definido                                                |

---

## üß† Cu√°ndo usar qu√©

| Escenario                                        | Mejor opci√≥n |
| ------------------------------------------------ | ------------ |
| Reacci√≥n simple al subir archivo                 | Lambda       |
| Procesamiento en cola por lotes                  | SQS          |
| Publicaci√≥n a m√∫ltiples sistemas                 | SNS          |
| Necesitas filtros complejos o m√∫ltiples destinos | EventBridge  |

---

## üß™ Ejemplo de caso real

> Se sube un archivo `.csv` al bucket `data-ingest`, y se debe:

- Validar el archivo con Lambda
- Enviar notificaci√≥n al equipo v√≠a SNS
- Registrar el evento en una base de eventos

üìå Soluci√≥n:

- S3 env√≠a eventos a **EventBridge**
- EventBridge enruta a:

  - Lambda para validaci√≥n
  - SNS para alerta
  - Firehose o DynamoDB v√≠a Lambda para almacenar eventos

---

## üßæ Resumen

| Tema                 | Detalle                                                                      |
| -------------------- | ---------------------------------------------------------------------------- |
| ¬øQu√© son?            | Eventos autom√°ticos que lanza S3 ante acciones sobre objetos                 |
| Destinos directos    | SNS, SQS, Lambda                                                             |
| Destino avanzado     | EventBridge                                                                  |
| EventBridge ventajas | Filtros JSON, m√∫ltiples destinos, repetici√≥n, archivado, DLQ                 |
| Configuraci√≥n        | Por consola, CLI o `NotificationConfiguration`                               |
| Casos comunes        | Subida de im√°genes, workflows de ingesta, validaciones, eventos distribuidos |
